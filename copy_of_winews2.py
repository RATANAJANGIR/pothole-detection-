# -*- coding: utf-8 -*-
"""Copy of winews2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gJ3i0ba6zhc9M_S3NOIHwiBr-Gsi38M
"""

##Of no use to you
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
!wget "https://www.sciencedirect.com/science/article/pii/S0167923609001377/pdfft?isDTMRedir=true&download=true"
## Link to paper that published this dataset :  https://reader.elsevier.com/reader/sd/pii/S0167923609001377?token=575E8254AB1294C5423422E33CCB3AF9634BF569E3DE21DEE2CB0B05EB613B307CC8F9DC64FCE91767615C7346131C9C

#Import Required Libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
from matplotlib import style

plt.ion()
style.use('ggplot')

#Import Dataset
white_dataset = pd.read_csv('./winequality-white.csv',header=0,sep=';')
help(white_dataset)

#Dataset Looks like this
white_dataset

white_dataset.describe()

print("Size of Dataset: ", white_dataset.shape)



##Separate Input and Output
input_data = white_dataset.iloc[:,:-1]
input_data

#Convert into numpy arrray and get mean of the data
X = np.array(input_data)
mu = np.mean(X,axis=0)
X.shape

#Separate Output
output_data = white_dataset.iloc[:,-1]
output_data

#Convert into numpy arrray
Y = np.array(output_data)

#BEWARE#
#SEE SHAPE BEFORE AND AFTER
Y.shape
Y = Y.reshape(-1,1)
Y.shape

print("Best Wine Possible : ",np.max(Y))
print("Worst Wine Possible : ",np.min(Y))
print("Average Wine Quality : ",round(np.mean(Y),3))

quality_count = {1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0}
for i in range(len(Y)):
    y = Y[i][0]
    quality_count[y] +=1

quality_count

plt.hist(Y,edgecolor='black')

for i in tqdm(range(11)):
    plt.figure(1,figsize=[15,25])
    plt.subplot(4,3,i+1)
    plt.title(white_dataset.columns[i])
    plt.hist(X[:,i],50,edgecolor='black',color='green')
plt.show()

X_normalized = X- mu
X_normalized = X_normalized/np.std(X_normalized,axis=0)

for i in tqdm(range(11)):
    plt.figure(1,figsize=[15,25])
    plt.subplot(4,3,i+1)
    plt.title(white_dataset.columns[i])
    #plt.xlim(-1,1)
    plt.hist(X_normalized[:,i],50,edgecolor='black',color='green')
plt.show()

correlation_matrix = white_dataset.corr()
correlation_matrix = correlation_matrix.abs()

correlation_matrix.style.background_gradient(cmap='coolwarm')

def split_dataset(X,Y,fraction=0.5):
    xtrain = []
    xtest = []
    ytrain = []
    ytest = []
    testlen = int(X.shape[0]*fraction)
    index = np.random.randint(0,len(Y)-testlen)

    xtest = X[index:index+testlen,:]
    ytest = Y[index:index+testlen]
    xtrain = np.concatenate((X[:index,:],X[index+testlen:,:]))
    ytrain = np.concatenate((Y[:index],Y[index+testlen:]))
    return xtrain,ytrain.reshape(-1,1),xtest,ytest.reshape(-1,1)

##Linear Features 
X_train, Y_train, X_test, Y_test = split_dataset(X_normalized, Y)

print("Train Dataset : ",X_train.shape," o :  ",Y_train.shape)
print("Train Dataset : ",X_test.shape," o :  ",Y_test.shape)

##Simple Linear Regression with SGD Optimizer

class linear_regressor():
    def __init__(self,input_size,epochs=20,learning_rate=0.1,batch_size=256):

        self.learning_rate = learning_rate
        #How many examples to update weights at a time
        self.batch_size=batch_size
        #input size of data
        self.input_size = input_size
        #how many time you want to iterate over whole data?
        self.epochs = epochs

        #initializing weights randomly
        self.weights = np.random.rand(input_size,1)
        #initializing bias (treated separately)
        self.bias = 0.0
    
    def forward(self,X,Y):
        #Forward pass of data to get output
        #y = W.x +b
        outputs = np.dot(X,self.weights) +self.bias
        self.error = outputs-Y
        return outputs
    
    def mse_loss(self):
        #Mean Square Loss .
        #Scaled by 0.5 to ease calculation later
        return np.mean(np.square(self.error))/2.0
    
    def compute_gradient(self,X):
        #compute gradient for weights and bias
        self.w_gradient =np.dot(np.transpose(X),self.error).reshape(-1,1)/X.shape[0]
        self.bias_gradient = np.mean(self.error)
    
    def update(self):
        #network update
        self.weights = self.weights - self.learning_rate*(self.w_gradient)
        self.bias = self.bias - self.learning_rate*(self.bias_gradient)
        
    def shuffle(self,X,Y):
        length = len(Y)
        indices = np.arange(length)
        np.random.shuffle(indices)
        return X[indices],Y[indices]


    def train(self,X,Y):
        for ep in range(self.epochs):
            x,y= self.shuffle(X,Y)
            losses = []
            for st_index in (range(0,len(Y),self.batch_size)):
                end_index = min(len(Y),st_index+self.batch_size)
                ##select batch for update
                x_batch = x[st_index:end_index]
                y_batch = y[st_index:end_index]
                #forward pass for prediction
                pred = self.forward(x_batch,y_batch)
                #loss aggregation
                losses.append(self.mse_loss())
                #compute loss using error
                self.compute_gradient(x_batch)
                #update parameter
                self.update()
            
            print("Epoch : ",ep)
            print("Average Mean Square Error over Epoch: ",np.mean(losses))

        
    def test(self,X,Y,tau):
        #tau decide how much error from actual value we can sustain
        losses = []
        correct = 0
        for st_index in (range(0,len(Y),self.batch_size)):
            end_index = min(len(Y),st_index+self.batch_size)
            x_batch = X[st_index:end_index]
            y_batch = Y[st_index:end_index]
            pred = self.forward(x_batch,y_batch)
            losses.append(self.mse_loss())
            for err in self.error:
                if abs(err)<=tau:
                    correct +=1
        print("Mean Square Error: ",np.mean(losses))
        print("Accuracy over test set  :",round(correct/len(Y),3))

##Regressor Creation
regressor1 = linear_regressor(X_train.shape[1])

regressor1.train(X_train,Y_train)

print("Over Train Set")
regressor1.test(X_train,Y_train,1)
print("Over Test Set")
regressor1.test(X_test,Y_test,1)

##Insider Look
##Anything

##Same Code As above just added Regularization parameter

class linear_regressor_regularize():
    #Alfa added
    def __init__(self,input_size,epochs=20,verbose=1,learning_rate=0.1,alfa = 0.001,batch_size=256):
        self.learning_rate = learning_rate
        self.batch_size=batch_size
        self.input_size = input_size
        self.epochs = epochs
        self.verbose = verbose
        #Regularization hyperparamter
        self.alfa = alfa
        self.weights = np.random.rand(input_size,1)
        self.bias = 0.0
    
    def forward(self,X,Y):
        outputs = np.dot(X,self.weights) +self.bias
        self.error = outputs-Y
        return outputs

    
    def mse_loss(self):
        return np.mean(np.square(self.error))/2.0
    
    def compute_reg_loss(self):
        #Regression Loss Computation
        self.reg_loss = np.linalg.vdot(self.weights,self.weights)/2

    def compute_gradient(self,X):
        self.w_gradient =np.dot(np.transpose(X),self.error).reshape(-1,1)/X.shape[0]
        self.bias_gradient = np.mean(self.error)
    
    def update(self):
        #Added Regression loss gradient along with normal gradient 
        self.weights = self.weights - (self.learning_rate*(self.w_gradient) - self.alfa*self.weights)
        self.bias = self.bias - self.learning_rate*(self.bias_gradient)
    
    def shuffle(self,X,Y):
        print(len(Y))
        length = len(Y)
        new_X = []
        new_Y = []
        indices = np.arange(length)
        np.random.shuffle(indices)
        for index in indices:
            new_X.append(X[index])
            new_Y.append(Y[index])

        return np.array(new_X).reshape(length,X.shape[1]), np.array(new_Y)
        
    
    def train(self,X,Y):
        for ep in range(self.epochs):
            x,y= self.shuffle(X,Y)
            losses = []
            for st_index in (range(0,len(Y),self.batch_size)):
                end_index = min(len(Y),st_index+self.batch_size)
                ##select batch for update
                x_batch = x[st_index:end_index]
                y_batch = y[st_index:end_index]
                #forward pass for prediction
                pred = self.forward(x_batch,y_batch)
                #loss aggregation
                losses.append(self.mse_loss())
                #compute loss using error
                self.compute_gradient(x_batch)
                #update parameter
                self.update()
            
            print("Epoch : ",ep)
            print("Average Mean Square Error over Epoch: ",np.mean(losses))

        
    def test(self,X,Y,tau):
        #tau decide how much error from actual value we can sustain
        losses = []
        correct = 0
        for st_index in (range(0,len(Y),self.batch_size)):
            end_index = min(len(Y),st_index+self.batch_size)
            x_batch = X[st_index:end_index]
            y_batch = Y[st_index:end_index]
            pred = self.forward(x_batch,y_batch)
            losses.append(self.mse_loss())
            for err in self.error:
                if abs(err)<=tau:
                    correct +=1
        print("Mean Square Error: ",np.mean(losses))
        print("Accuracy over test set  :",round(correct/len(Y),4))

regularized_regressor = linear_regressor_regularize(11)

regularized_regressor.train(X_train,Y_train)

print("Over Train Set")
regularized_regressor.test(X_train,Y_train,1)
print("Over Test Set")
regularized_regressor.test(X_test,Y_test,1)

##Principal Component Analysis

##Step1 Get Covatiance Matrix
covariance_matrix = np.cov(X_train,rowvar=False)

##Step2 Get Eigen Values and ACorresponding eigen values of covariance matrix
eig_val_cov, eig_vec_cov = np.linalg.eig(covariance_matrix)

#Eagen Values
eig_val_cov

#Take some of the highest eigen values say 2(here). 
#Taking first 2
t_mat = eig_vec_cov[:2]

#Shape of Transformation Matrix
t_mat.shape

##Transform dataset
x_train_reduced  = np.dot(X_train,np.transpose(t_mat))
x_test_reduced = np.dot(X_test,np.transpose(t_mat))

##Size of Transformed dataset
print("Train Dataset : ",x_train_reduced.shape," o :  ",Y_train.shape)
print("Train Dataset : ",x_test_reduced.shape," o :  ",Y_test.shape)

##Create our Regressor Instance
regressor2 = linear_regressor(2)

##Traing new Regressor
regressor2.train(x_train_reduced,Y_train)

print("Over Train Set")
regressor2.test(x_train_reduced,Y_train,1)
print("Over Test Set")
regressor2.test(x_test_reduced,Y_test,1)

#Using Libraries

"""# Using SKLEARN Library"""

from sklearn.model_selection import train_test_split

X_train, Y_train, X_test, Y_test = split_dataset(X_normalized, Y)

print("Train Dataset : ",X_train.shape," o :  ",Y_train.shape)
print("Train Dataset : ",X_test.shape," o :  ",Y_test.shape)

#1. LinearRegression with SGD
from sklearn import linear_model
model = linear_model.SGDRegressor(verbose=1,penalty='elasticnet')
model.fit(X_train,Y_train)

train_prediction = model.predict(X_train)
correct=0
total =0
for y in train_prediction:
    if abs(y-Y_train[total])<=1:
        correct+=1
    total+=1
print("Accuracy for Linear Regression on Training: ",correct/total)


test_prediction = model.predict(X_test)
correct=0
total =0
for y in test_prediction:
    if abs(y-Y_test[total])<=1:
        correct+=1
    total+=1
print("Accuracy for Linear Regression on testing: ",correct/total)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X_normalized)

print(pca.explained_variance_ratio_)
print(pca.singular_values_)

reduced_data = pca.transform(X_normalized)

rX_train, rY_train, rX_test, rY_test = split_dataset(reduced_data, Y)

model = linear_model.SGDRegressor(verbose=1)
model.fit(rX_train,rY_train)

train_prediction = model.predict(rX_train)
correct=0
total =0
for y in train_prediction:
    if abs(y-rY_train[total])<=1:
        correct+=1
    total+=1
print("Accuracy for Linear Regression on Training: ",correct/total)


test_prediction = model.predict(rX_test)
correct=0
total =0
for y in test_prediction:
    if abs(y-rY_test[total])<=1:
        correct+=1
    total+=1
print("Accuracy for Linear Regression on testing: ",correct/total)

!nvidia-smi

